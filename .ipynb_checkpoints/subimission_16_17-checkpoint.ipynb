{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decf2114",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "decf2114",
    "outputId": "28d58261-cac9-4740-9232-150ef8914dbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\users\\devan\\anaconda3\\lib\\site-packages (2.11.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\devan\\anaconda3\\lib\\site-packages (2.11.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-intel==2.11.0 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from tensorflow) (2.11.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\devan\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (21.3)\n",
      "Requirement already satisfied: keras<2.12,>=2.11.0 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (22.11.23)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.2.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.12.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (14.0.6)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (4.4.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.19.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.42.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.28.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\devan\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\devan\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (58.0.4)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.20.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.11.0->tensorflow) (0.37.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.6.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.0.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.26.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.2.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\devan\\appdata\\roaming\\python\\python39\\site-packages (from packaging->tensorflow-intel==2.11.0->tensorflow) (3.0.9)\n",
      "Requirement already satisfied: xgboost in c:\\users\\devan\\anaconda3\\lib\\site-packages (1.6.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\devan\\anaconda3\\lib\\site-packages (from xgboost) (1.7.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\devan\\anaconda3\\lib\\site-packages (from xgboost) (1.20.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras\n",
    "!pip install tensorflow\n",
    "!pip install xgboost\n",
    "!pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28c23b4",
   "metadata": {
    "id": "a28c23b4"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "%matplotlib inline \n",
    "\n",
    "## Models\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import XGBRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "import lightgbm as lgb \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "## Model evaluators\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import plot_roc_curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TwwGj7DEa2jW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TwwGj7DEa2jW",
    "outputId": "e1ead111-2921-48d5-f31d-abfb52618346"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "try:\n",
    "    import theano\n",
    "except:\n",
    "    !pip install Theano\n",
    "import theano\n",
    "import keras\n",
    "import tensorflow\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74129ff4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "74129ff4",
    "outputId": "8f147a8a-ea9b-4b34-8fa1-2c29e8de6dd2"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"TrainingData.csv\")\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb1fa41",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1cb1fa41",
    "outputId": "f43bad13-7a2c-4cc2-c92b-aef7ffec1ff2"
   },
   "outputs": [],
   "source": [
    "train_df1 = train_df.drop(['mvar47','application_key'], axis=1)\n",
    "\n",
    "train_df2 = train_df1.replace(to_replace =\"[a-zA-Z]+\", value = np.nan, regex = True)\n",
    "train_df2 = train_df2.astype('float')\n",
    "train_df2 = pd.concat([train_df2, train_df['mvar47']], axis=1)\n",
    "train_df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff25a19",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 456
    },
    "id": "9ff25a19",
    "outputId": "c356fda5-d364-49f8-981a-7f15ecc42ef2"
   },
   "outputs": [],
   "source": [
    "train_df2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mHqnuvOGhr6G",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mHqnuvOGhr6G",
    "outputId": "66919c2b-42a1-40da-be2a-61466718767c"
   },
   "outputs": [],
   "source": [
    "# one hot encoding \n",
    "train_df2 = pd.get_dummies(train_df2, columns=['mvar47'])\n",
    "train_df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HvjTeYEP8uqc",
   "metadata": {
    "id": "HvjTeYEP8uqc"
   },
   "outputs": [],
   "source": [
    "#def new_col_for_null(df, column):\n",
    "  #df[column+'_null'] = np.where(df[column].isnull(), 1, 0)\n",
    "  #return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0826iy5W_C2h",
   "metadata": {
    "id": "0826iy5W_C2h"
   },
   "outputs": [],
   "source": [
    "#for col in train_df2.columns:\n",
    "  #new_col_for_null(train_df2, col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0888ba72",
   "metadata": {
    "id": "0888ba72"
   },
   "source": [
    "# **Exploratory Data Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3369c3cd",
   "metadata": {
    "id": "3369c3cd"
   },
   "source": [
    "**Classes' Distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605d1653",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 346
    },
    "id": "605d1653",
    "outputId": "09737968-8de6-443d-9312-e5593a5efb5a"
   },
   "outputs": [],
   "source": [
    "# The frequency of defaults\n",
    "yes = train_df2.default_ind.sum()\n",
    "no = len(train_df2)-yes\n",
    "\n",
    "# Percentage\n",
    "yes_perc = round(yes/len(train_df2)*100, 1)\n",
    "no_perc = round(no/len(train_df2)*100, 1)\n",
    "\n",
    "import sys \n",
    "plt.figure(figsize=(7,4))\n",
    "sns.set_context('notebook', font_scale=1.2)\n",
    "sns.countplot('default_ind', data=train_df2, palette=\"Blues\")\n",
    "plt.annotate('Non-default: {}'.format(no), xy=(-0.3, 15000), xytext=(-0.3, 3000), size=12)\n",
    "plt.annotate('Default: {}'.format(yes), xy=(0.7, 15000), xytext=(0.7, 3000), size=12)\n",
    "plt.annotate(str(no_perc)+\" %\", xy=(-0.3, 15000), xytext=(-0.1, 8000), size=12)\n",
    "plt.annotate(str(yes_perc)+\" %\", xy=(0.7, 15000), xytext=(0.9, 8000), size=12)\n",
    "#Removing the frame\n",
    "plt.box(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vGqwwmhxzKaS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 574
    },
    "id": "vGqwwmhxzKaS",
    "outputId": "b1d81ed1-3c64-435f-fb52-2c762c9b43bf"
   },
   "outputs": [],
   "source": [
    "# this code helps us figure out if there is any relation between the missing values of two or more columns\n",
    "# whether it is a case of MACR or MAR\n",
    "\n",
    "import missingno as msno\n",
    "msno.heatmap(train_df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZqhZdn3fyyxP",
   "metadata": {
    "id": "ZqhZdn3fyyxP"
   },
   "source": [
    "#### **Train Test Split and then doing Feature Engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91M4enAB1ju1",
   "metadata": {
    "id": "91M4enAB1ju1"
   },
   "outputs": [],
   "source": [
    "X = train_df2.drop(['default_ind'], axis = 1)\n",
    "y = train_df2['default_ind']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, stratify=y)\n",
    "\n",
    "df_list = [X_train, X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UDxNDSL_i-GZ",
   "metadata": {
    "id": "UDxNDSL_i-GZ"
   },
   "outputs": [],
   "source": [
    "for df in df_list:\n",
    "  df['mvar7'] = df['mvar7'] + df['mvar8'] \n",
    "  df['mvar11_12'] = df['mvar11']/df['mvar12'] \n",
    "  mvar161718 = (df['mvar16'].add(df['mvar17'], fill_value=0)).add(df['mvar18'], fill_value=0)\n",
    "  df['mvar16'] = mvar161718\n",
    "  df['mvar19'] = df['mvar19'] + df['mvar20'] \n",
    "  df['mvar16_19'] = df['mvar16']/df['mvar19'] \n",
    "  df['mvar26'] = (df['mvar26']+df['mvar27'])\n",
    "  df['mvar30'] = df['mvar30'] + df['mvar31']\n",
    "  df['mvar30_32'] = (df['mvar30']/30) / df['mvar32']\n",
    "  df['mvar35'] = df['mvar35'] + df['mvar34']\n",
    "  df['mvar28_36'] = df['mvar28'] / df['mvar36']\n",
    "  df['mvar22_38'] = (df['mvar22']/100) * df['mvar38']\n",
    "  df['mvar39_36'] = df['mvar39'] / df['mvar36']\n",
    "  df['mvar34_39'] = df['mvar34'] / df['mvar39']\n",
    "  df['mvar43_36'] = df['mvar43'] / df['mvar36']\n",
    "  df['mvar43_38'] = df['mvar43'] / df['mvar38']\n",
    "  df['mvar45'] = (df['mvar45'] + df['mvar46'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "z6jZHhJ7tuER",
   "metadata": {
    "id": "z6jZHhJ7tuER"
   },
   "outputs": [],
   "source": [
    "#Since we have combined certain columns, we need to drop those which are already considered in the combination.\n",
    "\n",
    "X_train1 = X_train.drop(['mvar8', 'mvar17', 'mvar18', 'mvar20', 'mvar22', 'mvar27', 'mvar31', 'mvar34', 'mvar38', \n",
    "                         'mvar46', 'mvar48', 'mvar49'], axis=1)\n",
    "\n",
    "X_test1 = X_test.drop(['mvar8', 'mvar17', 'mvar18', 'mvar20', 'mvar22', 'mvar27', 'mvar31', 'mvar34', 'mvar38', \n",
    "                       'mvar46', 'mvar48', 'mvar49'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8928196f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "8928196f",
    "outputId": "9bb3bc81-578b-4c87-d90f-88c9298a2f04"
   },
   "outputs": [],
   "source": [
    "null_df = pd.DataFrame((X_train1.isnull().sum())*100/X_train1.shape[0]).reset_index()\n",
    "null_df.columns = ['Column Name', 'Null Values Percentage']\n",
    "\n",
    "# more than or equal to 40% empty rows columns\n",
    "nullcol_50_df = null_df[null_df[\"Null Values Percentage\"]>=50]\n",
    "nullcol_50_df.sort_values(by='Null Values Percentage', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "doyRVtkWsTD9",
   "metadata": {
    "id": "doyRVtkWsTD9"
   },
   "outputs": [],
   "source": [
    "# dropping columns which have > 70% NULL values\n",
    "X_train2 = X_train1.drop(['mvar30_32', 'mvar34_39', 'mvar30', 'mvar40'], axis=1)\n",
    "X_test2 = X_test1.drop(['mvar30_32', 'mvar34_39', 'mvar30', 'mvar40'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29364625",
   "metadata": {
    "id": "29364625"
   },
   "source": [
    "**Check Correlation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RES4HdG6b-yV",
   "metadata": {
    "id": "RES4HdG6b-yV"
   },
   "outputs": [],
   "source": [
    "X_comb = pd.DataFrame(pd.concat([X_train2, Y_train], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17LZJ1jS8dHa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "17LZJ1jS8dHa",
    "outputId": "4c9b620c-2ff2-41ba-9f8a-eb31bb7bc0ca"
   },
   "outputs": [],
   "source": [
    "#Correlation with output variable\n",
    "cor = X_comb.corr()\n",
    "cor_target = abs(cor[\"default_ind\"])\n",
    "#Selecting highly correlated features\n",
    "relevant_features = cor_target[cor_target>0.25]\n",
    "relevant_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ghjgI_8ttqUs",
   "metadata": {
    "id": "ghjgI_8ttqUs"
   },
   "outputs": [],
   "source": [
    "def print_highly_correlated(df, features, threshold=0.5):\n",
    "  corr_df = df[features].corr(method='spearman', min_periods = 3) # get correlations\n",
    "  correlated_features = np.where(np.abs(corr_df) > threshold) # select ones above the abs threshold\n",
    "  correlated_features = [(corr_df.iloc[x,y], x, y) for x, y in zip(*correlated_features) if x != y and x < y] # avoid duplication\n",
    "  s_corr_list = sorted(correlated_features, key=lambda x: -abs(x[0])) # sort by correlation value\n",
    "  \n",
    "  if s_corr_list == []:\n",
    "    print(\"There are no highly correlated features with correlation above\", threshold)\n",
    "  else:\n",
    "    for v, i, j in s_corr_list:\n",
    "      cols = df[features].columns\n",
    "      print (\"%s and %s = %.3f\" % (corr_df.index[i], corr_df.columns[j], v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9OJHBrS8viFD",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9OJHBrS8viFD",
    "outputId": "6524e5fd-fded-4c78-e098-9640a8d8e35e"
   },
   "outputs": [],
   "source": [
    "print_highly_correlated(X_comb, X_comb.columns, threshold=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2i2cJ2pLxV-_",
   "metadata": {
    "id": "2i2cJ2pLxV-_"
   },
   "source": [
    "*Variance Inflation factor*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bhyuJN9GxWpE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 519
    },
    "id": "bhyuJN9GxWpE",
    "outputId": "30e00c0e-6a4d-4491-bc73-d33106102655"
   },
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "# Indicate which variables to compute VIF\n",
    "Y = X_comb\n",
    "\n",
    "Y.dropna(inplace=True) # vif can't be calculated with nan values\n",
    "Y = Y._get_numeric_data()\n",
    "\n",
    "# Compute VIF\n",
    "vif = pd.DataFrame()\n",
    "vif[\"variables\"] = Y.columns\n",
    "vif[\"VIF\"] = [variance_inflation_factor(Y.values, i) for i in range(Y.shape[1])]\n",
    "vif_sorted = vif.sort_values(by='VIF', ascending=False)\n",
    "vif_sorted.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XcPgMrmZrWfI",
   "metadata": {
    "id": "XcPgMrmZrWfI"
   },
   "source": [
    "**Dropping Highly correlated and high VIF columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qSBp_YwgdIWX",
   "metadata": {
    "id": "qSBp_YwgdIWX"
   },
   "outputs": [],
   "source": [
    "X_train3 = X_train2.drop(['mvar10', 'mvar32', 'mvar28', 'mvar39', 'mvar16_19'], axis=1)\n",
    "X_test3 = X_test2.drop(['mvar10', 'mvar32', 'mvar28', 'mvar39', 'mvar16_19'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ejw32UKvymSi",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ejw32UKvymSi",
    "outputId": "545bb9b8-e19f-499c-db21-752a8493fe04"
   },
   "outputs": [],
   "source": [
    "X_train3.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4qFB9XxZrbcY",
   "metadata": {
    "id": "4qFB9XxZrbcY"
   },
   "outputs": [],
   "source": [
    "X_train3_1 = X_train3.replace([np.inf, -np.inf], 0)\n",
    "X_test3_1 = X_test3.replace([np.inf, -np.inf], 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1a331c",
   "metadata": {
    "id": "de1a331c"
   },
   "source": [
    "**Scaling and normalisation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94170c28",
   "metadata": {
    "id": "94170c28"
   },
   "source": [
    "it is necessary because we will be using knn imputation as well as SMOTE for oversampling, which is also based on KNN methodology. For this reason, we need to scale the data so that higher magnitudes don't led to formation of clusters when using KNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1KUaXyITfxlx",
   "metadata": {
    "id": "1KUaXyITfxlx"
   },
   "source": [
    "*Standardization*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360a71f0",
   "metadata": {
    "id": "360a71f0"
   },
   "outputs": [],
   "source": [
    "# importing sklearn StandardScaler class which is for Standardization\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#sc = StandardScaler() # creating an instance of the class object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fxYk3rTMh2dt",
   "metadata": {
    "id": "fxYk3rTMh2dt"
   },
   "source": [
    "*Robust Scaling*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rkaU4ezuhnsS",
   "metadata": {
    "id": "rkaU4ezuhnsS"
   },
   "outputs": [],
   "source": [
    "# importing sklearn Min Max Scaler class which is for Robust scaling\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "rs = RobustScaler() # creating an instance of the class object\n",
    "X_tr_sc = rs.fit_transform(X_train3_1)\n",
    "X_te_sc = rs.fit_transform(X_test3_1)\n",
    "Xtrain_scaled = pd.DataFrame(X_tr_sc, columns=X_train3_1.columns)\n",
    "Xtest_scaled = pd.DataFrame(X_te_sc, columns=X_test3_1.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iWFHdAoBg9WJ",
   "metadata": {
    "id": "iWFHdAoBg9WJ"
   },
   "source": [
    "**Imputing NULL values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zfxi0w6z2nE1",
   "metadata": {
    "id": "zfxi0w6z2nE1"
   },
   "outputs": [],
   "source": [
    "Xtrain_scaled1 = Xtrain_scaled.dropna(how=\"all\")\n",
    "Xtest_scaled1 = Xtest_scaled.dropna(how=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b50329e",
   "metadata": {
    "id": "6b50329e"
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=5, weights='distance')\n",
    "imputed_1 = imputer.fit_transform(Xtrain_scaled1)\n",
    "X_train3 = pd.DataFrame(imputed_1, columns=Xtrain_scaled1.columns)\n",
    "\n",
    "imputed_2 = imputer.fit_transform(Xtest_scaled1)\n",
    "X_test3 = pd.DataFrame(imputed_2, columns=Xtest_scaled1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1_HV_G9L6SDb",
   "metadata": {
    "id": "1_HV_G9L6SDb"
   },
   "outputs": [],
   "source": [
    "X_train4 = X_train3.drop('mvar47_C', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "z2NrArriw2Bs",
   "metadata": {
    "id": "z2NrArriw2Bs"
   },
   "outputs": [],
   "source": [
    "X_test4 = X_test3.drop('mvar47_C', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1D-9TdKc4zi",
   "metadata": {
    "id": "c1D-9TdKc4zi"
   },
   "source": [
    "## **Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q_90s5SH6b1t",
   "metadata": {
    "id": "q_90s5SH6b1t"
   },
   "source": [
    "#### **Dataset Balancing using SMOTETOMEK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rkrGtknVYlop",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rkrGtknVYlop",
    "outputId": "32e53e94-1cbf-4830-a69b-a9a05e776432"
   },
   "outputs": [],
   "source": [
    "# Since our classes are highly skewed we should make them equivalent in order to have a normal \n",
    "# distribution of the classes.\n",
    "\n",
    "from imblearn.combine import SMOTETomek \n",
    "smt = SMOTETomek(sampling_strategy=0.999, random_state=45, n_jobs=-1)\n",
    "X_sampled1, Y_sampled1 = smt.fit_resample(X_train4, Y_train)\n",
    "\n",
    "X_sampled1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3itNyU-zc4MB",
   "metadata": {
    "id": "3itNyU-zc4MB"
   },
   "outputs": [],
   "source": [
    "# sequential model to initialise our ann and dense module to build the layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CNuOszzOc4JD",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CNuOszzOc4JD",
    "outputId": "d3d2d192-4548-49e1-88ab-dec585af6cd5"
   },
   "outputs": [],
   "source": [
    "classifier1 = Sequential()\n",
    "# Adding the input layer and the first hidden layer\n",
    "classifier1.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 39))\n",
    "\n",
    "# Adding the second hidden layer\n",
    "classifier1.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "\n",
    "# Adding the output layer\n",
    "classifier1.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "\n",
    "# Compiling the ANN | means applying SGD on the whole ANN\n",
    "classifier1.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fitting the ANN to the Training set\n",
    "classifier1.fit(X_sampled1, Y_sampled1, batch_size = 10, epochs = 100,verbose = 0)\n",
    "\n",
    "score, acc = classifier1.evaluate(X_sampled1, Y_sampled1, batch_size=10)\n",
    "print('Train score:', score)\n",
    "print('Train accuracy:', acc)\n",
    "\n",
    "# Part 3 - Making predictions and evaluating the model\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier1.predict(X_test4)\n",
    "y_pred = (y_pred > 0.5)\n",
    "\n",
    "print('*'*20)\n",
    "score, acc = classifier1.evaluate(X_test4, y_test, batch_size=10)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3jFTXWi3MM_a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3jFTXWi3MM_a",
    "outputId": "a7722843-7de3-470d-e4ec-f5c9367f3d7b"
   },
   "outputs": [],
   "source": [
    "# Part 4 - Evaluating, Improving and Tuning the ANN\n",
    "\n",
    "# Evaluating the ANN\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "def build_classifier():\n",
    "    classifier = Sequential()\n",
    "    classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 39))\n",
    "    classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    return classifier\n",
    "classifier = KerasClassifier(build_fn = build_classifier, batch_size = 10, epochs = 100,verbose=0)\n",
    "accuracies = cross_val_score(estimator = classifier, X = X_sampled1, y = Y_sampled1, cv = 10)\n",
    "mean = accuracies.mean()\n",
    "variance = accuracies.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swdmPr0CMqiv",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "swdmPr0CMqiv",
    "outputId": "bf51031a-05a0-407c-d70f-9939b455b231"
   },
   "outputs": [],
   "source": [
    "print('Mean accuracy score of 10 different models using Kfold cross validation: {}'.format(mean))\n",
    "print('Standard Deviation of accuracy score of 10 different models using Kfold cross validation: {}'.format(variance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_77Iffkmu6ZL",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_77Iffkmu6ZL",
    "outputId": "41f8ff79-dfc3-47a5-e4e9-dd5be90c5af4"
   },
   "outputs": [],
   "source": [
    "# Improving ANN with Dropout layerÂ¶\n",
    "\n",
    "# Improving the ANN\n",
    "from keras.layers import Dropout\n",
    "classifier_improved = Sequential()\n",
    "# Adding the input layer and the first hidden layer\n",
    "classifier_improved.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 39))\n",
    "classifier_improved.add(Dropout(rate = 0.1))\n",
    "\n",
    "# Adding the second hidden layer\n",
    "classifier_improved.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "classifier_improved.add(Dropout(rate = 0.1))\n",
    "\n",
    "# Adding the output layer\n",
    "classifier_improved.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "\n",
    "# Compiling the ANN\n",
    "classifier_improved.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fitting the ANN to the Training set\n",
    "classifier_improved.fit(X_sampled1, Y_sampled1, batch_size = 10, epochs = 100,verbose = 0)\n",
    "\n",
    "# Part 3 - Making predictions and evaluating the model\n",
    "\n",
    "score, acc = classifier_improved.evaluate(X_sampled1, Y_sampled1, batch_size=10)\n",
    "print('Train score:', score)\n",
    "print('Train accuracy:', acc)\n",
    "# Part 3 - Making predictions and evaluating the model\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier_improved.predict(X_test4)\n",
    "y_pred = (y_pred > 0.5)\n",
    "\n",
    "print('*'*20)\n",
    "score, acc = classifier_improved.evaluate(X_test4, y_test, batch_size=10)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0PvnBlq-SRGf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 324
    },
    "id": "0PvnBlq-SRGf",
    "outputId": "ab33d985-21c6-4bea-9191-e53d8dab68bf"
   },
   "outputs": [],
   "source": [
    "p = sns.heatmap(pd.DataFrame(cm), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "plt.title('Confusion matrix', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raW8LOeVMqZ2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "raW8LOeVMqZ2",
    "outputId": "2b809ea0-1464-4809-9cae-eb925778f7d0"
   },
   "outputs": [],
   "source": [
    "#import classification_report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XtI72pCgz7GS",
   "metadata": {
    "id": "XtI72pCgz7GS"
   },
   "source": [
    "#### **Tuning the ANN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bGqH0AryASo",
   "metadata": {
    "id": "4bGqH0AryASo"
   },
   "outputs": [],
   "source": [
    "# Tuning the ANN\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "def build_classifier(optimizer):\n",
    "    classifier = Sequential()\n",
    "    classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 39))\n",
    "    classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "    classifier.compile(optimizer = optimizer, loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    return classifier\n",
    "\n",
    "classifier = KerasClassifier(build_fn = build_classifier)\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "parameters = {'batch_size': [25, 32], 'epochs': [100, 200], 'optimizer': ['adam', 'rmsprop']}\n",
    "rs = RandomizedSearchCV(estimator = classifier, param_distributions = parameters, scoring = 'accuracy',\n",
    "                           cv = cv, n_jobs = -1)\n",
    "rs_result = rs.fit(X_sampled1, Y_sampled1, verbose = 0)\n",
    "best_parameters = rs_result.best_params_\n",
    "best_accuracy = rs_result.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954818c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best Parameters after tuning: {}'.format(best_parameters))\n",
    "print('Best Accuracy after tuning: {}'.format(best_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WrNsU4MbjqpR",
   "metadata": {
    "id": "WrNsU4MbjqpR"
   },
   "source": [
    "# **Working on COMPANY PROVIDED TEST DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb9a9cb",
   "metadata": {
    "id": "5eb9a9cb"
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"testX.csv\")\n",
    "test_df1 = test_df.drop(['mvar47'], axis=1)\n",
    "test_df2 = test_df1.replace(to_replace =\"[a-zA-Z]+\", value = np.nan, regex = True)\n",
    "test_df2 = test_df2.astype('float')\n",
    "test_df2 = pd.concat([test_df2, test_df['mvar47']], axis=1)\n",
    "test_df2 = pd.get_dummies(test_df2, columns=['mvar47'])\n",
    "df = test_df2.drop('application_key', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kXQqNmNsSP4P",
   "metadata": {
    "id": "kXQqNmNsSP4P"
   },
   "outputs": [],
   "source": [
    "df['mvar7'] = df['mvar7'] + df['mvar8'] \n",
    "df['mvar11_12'] = df['mvar11']/df['mvar12'] \n",
    "mvar161718 = (df['mvar16'].add(df['mvar17'], fill_value=0)).add(df['mvar18'], fill_value=0)\n",
    "df['mvar16'] = mvar161718\n",
    "df['mvar19'] = df['mvar19'] + df['mvar20'] \n",
    "df['mvar16_19'] = df['mvar16']/df['mvar19'] \n",
    "df['mvar26'] = (df['mvar26']+df['mvar27'])\n",
    "df['mvar30'] = df['mvar30'] + df['mvar31']\n",
    "df['mvar30_32'] = (df['mvar30']/30) / df['mvar32']\n",
    "df['mvar35'] = df['mvar35'] + df['mvar34']\n",
    "df['mvar28_36'] = df['mvar28'] / df['mvar36']\n",
    "df['mvar22_38'] = (df['mvar22']/100) * df['mvar38']\n",
    "df['mvar39_36'] = df['mvar39'] / df['mvar36']\n",
    "df['mvar34_39'] = df['mvar34'] / df['mvar39']\n",
    "df['mvar43_36'] = df['mvar43'] / df['mvar36']\n",
    "df['mvar43_38'] = df['mvar43'] / df['mvar38']\n",
    "df['mvar45'] = (df['mvar45'] + df['mvar46'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GGYD156CSP4T",
   "metadata": {
    "id": "GGYD156CSP4T"
   },
   "outputs": [],
   "source": [
    "#Since we have combined certain columns, we need to drop those which are already considered in the combination.\n",
    "\n",
    "df1 = df.drop(['mvar8', 'mvar17', 'mvar18', 'mvar20', 'mvar22', 'mvar27', 'mvar31', 'mvar34', 'mvar38', \n",
    "                         'mvar46', 'mvar48', 'mvar49'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7olnDMrTSP4U",
   "metadata": {
    "id": "7olnDMrTSP4U"
   },
   "outputs": [],
   "source": [
    "# dropping columns which have > 70% NULL values\n",
    "df2 = df1.drop(['mvar30_32', 'mvar34_39', 'mvar30', 'mvar40'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SeFqhuUKSP4X",
   "metadata": {
    "id": "SeFqhuUKSP4X"
   },
   "source": [
    "**Dropping Highly correlated and high VIF columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Df1f1fftSP4X",
   "metadata": {
    "id": "Df1f1fftSP4X"
   },
   "outputs": [],
   "source": [
    "df3 = df2.drop(['mvar10', 'mvar32', 'mvar28', 'mvar39', 'mvar16_19'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2wHeL4r6SP4X",
   "metadata": {
    "id": "2wHeL4r6SP4X"
   },
   "outputs": [],
   "source": [
    "df4 = df3.replace([np.inf, -np.inf], np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aZVyZYVPSP4X",
   "metadata": {
    "id": "aZVyZYVPSP4X"
   },
   "source": [
    "**Scaling and normalisation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "odrB3jsCSP4Y",
   "metadata": {
    "id": "odrB3jsCSP4Y"
   },
   "source": [
    "it is necessary because we will be using knn imputation as well as SMOTE for oversampling, which is also based on KNN methodology. For this reason, we need to scale the data so that higher magnitudes don't led to formation of clusters when using KNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LgGchIGiSP4Y",
   "metadata": {
    "id": "LgGchIGiSP4Y"
   },
   "source": [
    "*Standardization*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yVQMsljqSP4Y",
   "metadata": {
    "id": "yVQMsljqSP4Y"
   },
   "outputs": [],
   "source": [
    "# importing sklearn StandardScaler class which is for Standardization\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#sc = StandardScaler() # creating an instance of the class object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesEIimFSP4Y",
   "metadata": {
    "id": "lesEIimFSP4Y"
   },
   "source": [
    "*Robust Scaling*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZJiSLcwcSP4Y",
   "metadata": {
    "id": "ZJiSLcwcSP4Y"
   },
   "outputs": [],
   "source": [
    "# importing sklearn Min Max Scaler class which is for Robust scaling\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "rs = RobustScaler() # creating an instance of the class object\n",
    "df_sc = rs.fit_transform(df4)\n",
    "df_scaled = pd.DataFrame(df_sc, columns=df4.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1T0d_pSP4Z",
   "metadata": {
    "id": "8b1T0d_pSP4Z"
   },
   "source": [
    "**Imputing NULL values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FpPs_Sh2SP4Z",
   "metadata": {
    "id": "FpPs_Sh2SP4Z"
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=5, weights='distance')\n",
    "imputed = imputer.fit_transform(df_scaled)\n",
    "test_df4 = pd.DataFrame(imputed, columns=df_scaled.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "H9ploMZ4zBEQ",
   "metadata": {
    "id": "H9ploMZ4zBEQ"
   },
   "outputs": [],
   "source": [
    "test_df4 = test_df4.drop('mvar47_C', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WDqMV400sTAp",
   "metadata": {
    "id": "WDqMV400sTAp"
   },
   "outputs": [],
   "source": [
    "# make predictions for test data\n",
    "X_test = test_df4\n",
    "y_pred = rs_result.predict(test_df4)\n",
    "\n",
    "new = pd.DataFrame(y_pred, columns=['predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UR8K1-8caBoT",
   "metadata": {
    "id": "UR8K1-8caBoT"
   },
   "outputs": [],
   "source": [
    "new.insert(0, \"application_key\", test_df['application_key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "w2_IMZ-9_9qm",
   "metadata": {
    "id": "w2_IMZ-9_9qm"
   },
   "outputs": [],
   "source": [
    "new = new.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "r0Ks_e0F3J6p",
   "metadata": {
    "id": "r0Ks_e0F3J6p"
   },
   "outputs": [],
   "source": [
    "new.to_csv('Daring_souls_20.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
