{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "decf2114",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "decf2114",
    "outputId": "3b3946d1-1eff-4eac-8adb-c43d0e939d12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\users\\devan\\anaconda3\\lib\\site-packages (2.11.0)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: tensorflow in c:\\users\\devan\\anaconda3\\lib\\site-packages (2.11.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.11.0 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from tensorflow) (2.11.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.19.1)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.2.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\devan\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (21.3)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.42.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.12.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (4.4.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (14.0.6)\n",
      "Requirement already satisfied: keras<2.12,>=2.11.0 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\devan\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (58.0.4)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.22.4)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (22.11.23)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.28.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\devan\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.11.0->tensorflow) (0.37.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.26.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.0.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.6.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.26.7)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.2.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\devan\\appdata\\roaming\\python\\python39\\site-packages (from packaging->tensorflow-intel==2.11.0->tensorflow) (3.0.9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\devan\\anaconda3\\lib\\site-packages (1.6.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\devan\\anaconda3\\lib\\site-packages (from xgboost) (1.7.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\devan\\anaconda3\\lib\\site-packages (from xgboost) (1.22.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: catboost in c:\\users\\devan\\anaconda3\\lib\\site-packages (1.1.1)\n",
      "Requirement already satisfied: pandas>=0.24.0 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from catboost) (1.3.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\devan\\appdata\\roaming\\python\\python39\\site-packages (from catboost) (3.6.2)\n",
      "Requirement already satisfied: graphviz in c:\\users\\devan\\anaconda3\\lib\\site-packages (from catboost) (0.20.1)\n",
      "Requirement already satisfied: plotly in c:\\users\\devan\\anaconda3\\lib\\site-packages (from catboost) (5.9.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\devan\\anaconda3\\lib\\site-packages (from catboost) (1.7.1)\n",
      "Requirement already satisfied: six in c:\\users\\devan\\appdata\\roaming\\python\\python39\\site-packages (from catboost) (1.16.0)\n",
      "Requirement already satisfied: numpy>=1.16.0 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from catboost) (1.22.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\devan\\appdata\\roaming\\python\\python39\\site-packages (from pandas>=0.24.0->catboost) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from pandas>=0.24.0->catboost) (2021.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\devan\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib->catboost) (4.38.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\devan\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib->catboost) (1.0.6)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\devan\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib->catboost) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\devan\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib->catboost) (3.0.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\devan\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib->catboost) (21.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\devan\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib->catboost) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\devan\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib->catboost) (9.3.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\devan\\anaconda3\\lib\\site-packages (from plotly->catboost) (8.0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\devan\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras\n",
    "!pip install tensorflow\n",
    "!pip install xgboost\n",
    "!pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a28c23b4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 789
    },
    "id": "a28c23b4",
    "outputId": "58bc4afd-dd33-40de-dc4e-84795c12917e"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "%matplotlib inline \n",
    "\n",
    "## Models\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "import lightgbm as lgb \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "## Model evaluators\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import plot_roc_curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Bl87HAtbXyG3",
   "metadata": {
    "id": "Bl87HAtbXyG3"
   },
   "outputs": [],
   "source": [
    " !pip install numba --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DNhkeQnTZ3DS",
   "metadata": {
    "id": "DNhkeQnTZ3DS"
   },
   "outputs": [],
   "source": [
    "!pip install imbalanced-learn==0.7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "I_zYZ2VjYZLq",
   "metadata": {
    "id": "I_zYZ2VjYZLq"
   },
   "outputs": [],
   "source": [
    "!pip install --user -U scikit-learn==0.23.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zE8dkrWSOvhG",
   "metadata": {
    "id": "zE8dkrWSOvhG"
   },
   "outputs": [],
   "source": [
    "!pip install pycaret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74129ff4",
   "metadata": {
    "id": "74129ff4"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"TrainingData.csv\")\n",
    "dataDict = pd.read_csv('Data_Dictionary.csv', na_values=\"NA\")\n",
    "\n",
    "print(train_df.shape)\n",
    "print(dataDict.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "U62nGvyshAJT",
   "metadata": {
    "id": "U62nGvyshAJT"
   },
   "outputs": [],
   "source": [
    "dic = dict(zip(dataDict.values[:, 1], dataDict.values[:, 0]))\n",
    "dic['Compound feature created as a product of bucketized credit worthiness and mvar48']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb1fa41",
   "metadata": {
    "id": "1cb1fa41"
   },
   "outputs": [],
   "source": [
    "train_df1 = train_df.drop(['mvar47'], axis=1)\n",
    "\n",
    "train_df2 = train_df1.replace(to_replace =\"[a-zA-Z]+\", value = np.nan, regex = True)\n",
    "train_df2 = train_df2.astype('float')\n",
    "train_df2 = pd.concat([train_df2, train_df['mvar47']], axis=1)\n",
    "train_df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff25a19",
   "metadata": {
    "id": "9ff25a19"
   },
   "outputs": [],
   "source": [
    "train_df2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mHqnuvOGhr6G",
   "metadata": {
    "id": "mHqnuvOGhr6G"
   },
   "outputs": [],
   "source": [
    "#one hot encoding \n",
    "train_df2 = pd.get_dummies(train_df2, columns = ['mvar47'])\n",
    "train_df2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0888ba72",
   "metadata": {
    "id": "0888ba72"
   },
   "source": [
    "# **Exploratory Data Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34562a76",
   "metadata": {
    "id": "34562a76"
   },
   "outputs": [],
   "source": [
    "train_df2.boxplot(column = 'mvar3',by = 'default_ind')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mEVjxUw9imeN",
   "metadata": {
    "id": "mEVjxUw9imeN"
   },
   "outputs": [],
   "source": [
    "train_df2.boxplot(column = 'mvar4',by = 'default_ind')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Wv7iYMv8inTs",
   "metadata": {
    "id": "Wv7iYMv8inTs"
   },
   "outputs": [],
   "source": [
    "train_df2['mvar3'] = (1+train_df2['mvar3'])*(1+train_df2['mvar4'])*(1+train_df2['mvar5'])\n",
    "train_df2.boxplot(column = 'mvar3',by = 'default_ind')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UDxNDSL_i-GZ",
   "metadata": {
    "id": "UDxNDSL_i-GZ"
   },
   "outputs": [],
   "source": [
    "train_df2['mvar7'][train_df2['mvar7'].isnull()] = -1 \n",
    "train_df2['mvar8'][train_df2['mvar8'].isnull()] = -1\n",
    "\n",
    "train_df2['mvar7'] = train_df2['mvar7'] + train_df2['mvar8'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pzfmLQnvi965",
   "metadata": {
    "id": "pzfmLQnvi965"
   },
   "outputs": [],
   "source": [
    "train_df2['default_ind'][train_df2['mvar7']>=60000].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MqhGo1QCkugU",
   "metadata": {
    "id": "MqhGo1QCkugU"
   },
   "outputs": [],
   "source": [
    "train_df2.boxplot(column = 'mvar7',by = 'default_ind')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Taj6JOdZkzSu",
   "metadata": {
    "id": "Taj6JOdZkzSu"
   },
   "outputs": [],
   "source": [
    "train_df2.boxplot(column = 'mvar8',by = 'default_ind')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kFRfSM4NkzDP",
   "metadata": {
    "id": "kFRfSM4NkzDP"
   },
   "outputs": [],
   "source": [
    "train_df2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xe_uIgmaky5O",
   "metadata": {
    "id": "xe_uIgmaky5O"
   },
   "outputs": [],
   "source": [
    "train_df2['mvar16'][train_df2['mvar16'].isnull()] = -1\n",
    "train_df2['mvar17'][train_df2['mvar17'].isnull()] = -1\n",
    "train_df2['mvar18'][train_df2['mvar18'].isnull()] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0zcbLOOVoQfo",
   "metadata": {
    "id": "0zcbLOOVoQfo"
   },
   "outputs": [],
   "source": [
    "train_df2['mvar16'] = train_df2['mvar16']+train_df2['mvar17']+train_df2['mvar18']\n",
    "\n",
    "train_df2['mvar16'][train_df2['mvar16']>1] = train_df2['mvar16'][train_df2['mvar16']>1] //5\n",
    "train_df2['mvar16'] = train_df2['mvar16'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ekMWZFGLoQY-",
   "metadata": {
    "id": "ekMWZFGLoQY-"
   },
   "outputs": [],
   "source": [
    "#mvar26\n",
    "\n",
    "train_df2['mvar26'][train_df2['mvar26'].isnull()==False]  = ((train_df2['mvar26'][train_df2['mvar26'].isnull()==False]).astype('int64'))//12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WpfCIMaHoQWb",
   "metadata": {
    "id": "WpfCIMaHoQWb"
   },
   "outputs": [],
   "source": [
    "# df['mvar26'][df['mvar26'].isnull()] = np.mean(df['mvar26'])\n",
    "train_df2['mvar26'][train_df2['mvar26'].isnull()] = -365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7QChsOnEoQT-",
   "metadata": {
    "id": "7QChsOnEoQT-"
   },
   "outputs": [],
   "source": [
    "train_df2['mvar26'] = train_df2['mvar26'].astype('int64')\n",
    "train_df2['mvar26'] = train_df2['mvar26']//365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97giQDeLoQRt",
   "metadata": {
    "id": "97giQDeLoQRt"
   },
   "outputs": [],
   "source": [
    "# mvar27\n",
    "\n",
    "train_df2['mvar27'][train_df2['mvar27'].isnull()] = -365\n",
    "train_df2['mvar27'] = train_df2['mvar27'] //365\n",
    "train_df2['mvar27'] = train_df2['mvar27'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FF9-9eyjoQPC",
   "metadata": {
    "id": "FF9-9eyjoQPC"
   },
   "outputs": [],
   "source": [
    "#mvar26+27\n",
    "\n",
    "train_df2['mvar27'][train_df2['mvar27'].isnull()] = -365\n",
    "train_df2['mvar26'] = (train_df2['mvar26']+train_df2['mvar27'])/2/365\n",
    "train_df2['mvar26'] = train_df2['mvar26'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bzq5YGKoQNF",
   "metadata": {
    "id": "6bzq5YGKoQNF"
   },
   "outputs": [],
   "source": [
    "#mvar28\n",
    "\n",
    "train_df2['mvar28'][train_df2['mvar28'].isnull()] = -365\n",
    "train_df2['mvar28'] = train_df2['mvar28'] //365\n",
    "train_df2['mvar28'] = train_df2['mvar28'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "I5sHKXoVpn9O",
   "metadata": {
    "id": "I5sHKXoVpn9O"
   },
   "outputs": [],
   "source": [
    "#mvar30\n",
    "\n",
    "train_df2['mvar30'][train_df2['mvar30'].isnull()] = -365\n",
    "train_df2['mvar30'] = train_df2['mvar30'] //365\n",
    "train_df2['mvar30'] = train_df2['mvar30'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LjbW_OGwpn60",
   "metadata": {
    "id": "LjbW_OGwpn60"
   },
   "outputs": [],
   "source": [
    "# mvar31\n",
    "\n",
    "train_df2['mvar31'][train_df2['mvar31'].isnull()] = -365\n",
    "train_df2['mvar31'] = train_df2['mvar31'] //365\n",
    "train_df2['mvar31'] = train_df2['mvar31'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yqL4qFp3pn4f",
   "metadata": {
    "id": "yqL4qFp3pn4f"
   },
   "outputs": [],
   "source": [
    "train_df2['mvar30'] = train_df2[[\"mvar30\", \"mvar31\"]].max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oj6smWW_qW-i",
   "metadata": {
    "id": "oj6smWW_qW-i"
   },
   "outputs": [],
   "source": [
    "train_df2['mvar30'][train_df2['mvar30'].isnull()] = -365\n",
    "train_df2['mvar30'] = train_df2['mvar30'] //365\n",
    "train_df2['mvar30'] = train_df2['mvar30'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "P6RUJLp2qW7f",
   "metadata": {
    "id": "P6RUJLp2qW7f"
   },
   "outputs": [],
   "source": [
    "#mvar33\n",
    "\n",
    "train_df2['mvar33'][train_df2['mvar33'].isnull()] = -5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cZZ-8s2pqW3q",
   "metadata": {
    "id": "cZZ-8s2pqW3q"
   },
   "outputs": [],
   "source": [
    "train_df2['mvar33'] = train_df2['mvar33'] //5\n",
    "train_df2['mvar33'] = train_df2['mvar33'].round(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08gdh3W5qWzd",
   "metadata": {
    "id": "08gdh3W5qWzd"
   },
   "outputs": [],
   "source": [
    "#mvar34\n",
    "\n",
    "train_df2['mvar34'][train_df2['mvar34'].isnull()] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_UkV-x1LqWw2",
   "metadata": {
    "id": "_UkV-x1LqWw2"
   },
   "outputs": [],
   "source": [
    "#mvar35\n",
    "\n",
    "train_df2['mvar35'][train_df2['mvar35'].isnull()] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BLS2R906qWuU",
   "metadata": {
    "id": "BLS2R906qWuU"
   },
   "outputs": [],
   "source": [
    "train_df2['mvar35'] = train_df2['mvar35'] + train_df2['mvar34']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0q63e1UStSo7",
   "metadata": {
    "id": "0q63e1UStSo7"
   },
   "outputs": [],
   "source": [
    "#mvar36\n",
    "\n",
    "train_df2['mvar36'][train_df2['mvar36'].isnull()] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-4Xl_xJVtSmH",
   "metadata": {
    "id": "-4Xl_xJVtSmH"
   },
   "outputs": [],
   "source": [
    "#mvar37\n",
    "\n",
    "train_df2['mvar37'][train_df2['mvar37'].isnull()] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "R5R8YxVHtSiq",
   "metadata": {
    "id": "R5R8YxVHtSiq"
   },
   "outputs": [],
   "source": [
    "#mvar38\n",
    "\n",
    "train_df2['mvar38'][train_df2['mvar38'].isnull()] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4XHrHOFEtSgX",
   "metadata": {
    "id": "4XHrHOFEtSgX"
   },
   "outputs": [],
   "source": [
    "#mvar39\n",
    "\n",
    "train_df2['mvar39'][train_df2['mvar39'].isnull()] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "z9MAtxR0t_6S",
   "metadata": {
    "id": "z9MAtxR0t_6S"
   },
   "outputs": [],
   "source": [
    "#mvar40\n",
    "\n",
    "binn = 50\n",
    "train_df2['mvar40'][train_df2['mvar40'].isnull()] = -binn\n",
    "train_df2['mvar40'] = train_df2['mvar40'] /binn\n",
    "train_df2['mvar40'] = train_df2['mvar40'].round(0)\n",
    "train_df2['mvar40'] = train_df2['mvar40'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vP0M11C3t_3V",
   "metadata": {
    "id": "vP0M11C3t_3V"
   },
   "outputs": [],
   "source": [
    "#mvar41\n",
    "\n",
    "binn = 25\n",
    "train_df2['mvar41'][train_df2['mvar41'].isnull()] = -binn\n",
    "train_df2['mvar41'] = train_df2['mvar41'] /binn\n",
    "train_df2['mvar41'] = train_df2['mvar41'].round(0)\n",
    "train_df2['mvar41'] = train_df2['mvar41'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6nV-s4j3t_vd",
   "metadata": {
    "id": "6nV-s4j3t_vd"
   },
   "outputs": [],
   "source": [
    "#mvar45\n",
    "\n",
    "# binn = 25\n",
    "train_df2['mvar45'][train_df2['mvar45'].isnull()] = -1\n",
    "# df2['mvar41'] = df2['mvar41'] /binn\n",
    "# df2['mvar41'] = df2['mvar41'].round(0)\n",
    "# df2['mvar41'] = df2['mvar41'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MWjJbEX3t_qu",
   "metadata": {
    "id": "MWjJbEX3t_qu"
   },
   "outputs": [],
   "source": [
    "#mvar46\n",
    "\n",
    "# binn = 25\n",
    "train_df2['mvar46'][train_df2['mvar46'].isnull()] = -1\n",
    "# df2['mvar41'] = df2['mvar41'] /binn\n",
    "# df2['mvar41'] = df2['mvar41'].round(0)\n",
    "# df2['mvar41'] = df2['mvar41'].astype('int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3369c3cd",
   "metadata": {
    "id": "3369c3cd"
   },
   "source": [
    "**Classes' Distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605d1653",
   "metadata": {
    "id": "605d1653"
   },
   "outputs": [],
   "source": [
    "# The frequency of defaults\n",
    "yes = train_df2.default_ind.sum()\n",
    "no = len(train_df2)-yes\n",
    "\n",
    "# Percentage\n",
    "yes_perc = round(yes/len(train_df2)*100, 1)\n",
    "no_perc = round(no/len(train_df2)*100, 1)\n",
    "\n",
    "import sys \n",
    "plt.figure(figsize=(7,4))\n",
    "sns.set_context('notebook', font_scale=1.2)\n",
    "sns.countplot('default_ind', data=train_df2, palette=\"Blues\")\n",
    "plt.annotate('Non-default: {}'.format(no), xy=(-0.3, 15000), xytext=(-0.3, 3000), size=12)\n",
    "plt.annotate('Default: {}'.format(yes), xy=(0.7, 15000), xytext=(0.7, 3000), size=12)\n",
    "plt.annotate(str(no_perc)+\" %\", xy=(-0.3, 15000), xytext=(-0.1, 8000), size=12)\n",
    "plt.annotate(str(yes_perc)+\" %\", xy=(0.7, 15000), xytext=(0.9, 8000), size=12)\n",
    "#Removing the frame\n",
    "plt.box(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zwUazw1Ywyfw",
   "metadata": {
    "id": "zwUazw1Ywyfw"
   },
   "outputs": [],
   "source": [
    "for i in [11, 23, 30, 31, 15, 35, 40, 41, 6, 8, 12, 13, 16, 17, 18, 21, 22, 24, 25, 26, 27, 32, 37, 39, 44, 46]:\n",
    "    train_df2.boxplot(column = 'mvar'+str(i),by = 'default_ind')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8928196f",
   "metadata": {
    "id": "8928196f"
   },
   "outputs": [],
   "source": [
    "null_df = pd.DataFrame((train_df2.isnull().sum())*100/train_df2.shape[0]).reset_index()\n",
    "null_df.columns = ['Column Name', 'Null Values Percentage']\n",
    "\n",
    "# more than or equal to 40% empty rows columns\n",
    "nullcol_50_df = null_df[null_df[\"Null Values Percentage\"]>=50]\n",
    "nullcol_50_df.sort_values(by='Null Values Percentage', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcab637",
   "metadata": {
    "id": "4fcab637"
   },
   "outputs": [],
   "source": [
    "# dropping highly correlated columns \n",
    "#train_df3 = train_df3.drop(['mvar18', 'mvar20', 'mvar23', 'mvar27', 'mvar3', 'mvar10', \n",
    "#'mvar32', 'mvar48', 'mvar49', 'mvar51'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SnywowXRv9C5",
   "metadata": {
    "id": "SnywowXRv9C5"
   },
   "outputs": [],
   "source": [
    "#dropping application id column\n",
    "train_df3 = train_df2.drop(['application_key'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1a331c",
   "metadata": {
    "id": "de1a331c"
   },
   "source": [
    "**Scaling and normalisation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94170c28",
   "metadata": {
    "id": "94170c28"
   },
   "source": [
    "it is necessary because we will be using knn imputation as well as SMOTE for oversampling, which is also based on KNN methodology. For this reason, we need to scale the data so that higher magnitudes don't led to formation of clusters when using KNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1KUaXyITfxlx",
   "metadata": {
    "id": "1KUaXyITfxlx"
   },
   "source": [
    "*Standardization*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360a71f0",
   "metadata": {
    "id": "360a71f0"
   },
   "outputs": [],
   "source": [
    "# importing sklearn StandardScaler class which is for Standardization\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#sc = StandardScaler() # creating an instance of the class object\n",
    "#df_scaled = pd.DataFrame(sc.fit_transform(train_df4), columns=train_df4.columns)  #fit and transforming StandardScaler the dataframe "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Bv-JLPJWgs6f",
   "metadata": {
    "id": "Bv-JLPJWgs6f"
   },
   "source": [
    "*Min-Max Scaling*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6t7HVxM3g73p",
   "metadata": {
    "id": "6t7HVxM3g73p"
   },
   "outputs": [],
   "source": [
    "# importing sklearn Min Max Scaler class which is for Standardization\n",
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#mm = MinMaxScaler() # creating an instance of the class object\n",
    "#df_scaled = pd.DataFrame(mm.fit_transform(train_df4), columns=train_df4.columns)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UD95N18AhdaG",
   "metadata": {
    "id": "UD95N18AhdaG"
   },
   "source": [
    "*Max Absolute Scaling*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-5e0hvxmhnvW",
   "metadata": {
    "id": "-5e0hvxmhnvW"
   },
   "outputs": [],
   "source": [
    "#from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "#ma = MaxAbsScaler() # creating an instance of the class object\n",
    "#df_scaled = pd.DataFrame(ma.fit_transform(train_df4), columns=train_df4.columns)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fxYk3rTMh2dt",
   "metadata": {
    "id": "fxYk3rTMh2dt"
   },
   "source": [
    "*Robust Scaling*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rkaU4ezuhnsS",
   "metadata": {
    "id": "rkaU4ezuhnsS"
   },
   "outputs": [],
   "source": [
    "# importing sklearn Min Max Scaler class which is for Robust scaling\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "rs = RobustScaler() # creating an instance of the class object\n",
    "X = rs.fit_transform(train_df3)\n",
    "df_scaled = pd.DataFrame(X, columns=train_df3.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iWFHdAoBg9WJ",
   "metadata": {
    "id": "iWFHdAoBg9WJ"
   },
   "source": [
    "**Imputing NULL values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pNan23Zhm6FQ",
   "metadata": {
    "id": "pNan23Zhm6FQ"
   },
   "outputs": [],
   "source": [
    "df_scaled1 = df_scaled.drop(['mvar39'], axis=1)\n",
    "df_scaled2 = df_scaled[['mvar39']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YtCPhoJ_nQIs",
   "metadata": {
    "id": "YtCPhoJ_nQIs"
   },
   "outputs": [],
   "source": [
    "result = df_scaled2.fillna(df_scaled2.mode().iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b50329e",
   "metadata": {
    "id": "6b50329e"
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=3)\n",
    "imputed = imputer.fit_transform(df_scaled1)\n",
    "train_data1 = pd.DataFrame(imputed, columns=df_scaled1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8xGh4A37n8N5",
   "metadata": {
    "id": "8xGh4A37n8N5"
   },
   "outputs": [],
   "source": [
    "train_data = pd.concat([train_data1, result], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OOGvWbJgtbyn",
   "metadata": {
    "id": "OOGvWbJgtbyn"
   },
   "outputs": [],
   "source": [
    "train_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29364625",
   "metadata": {
    "id": "29364625"
   },
   "source": [
    "**Check Correlation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17LZJ1jS8dHa",
   "metadata": {
    "id": "17LZJ1jS8dHa"
   },
   "outputs": [],
   "source": [
    "#Correlation with output variable\n",
    "cor = train_data.corr()\n",
    "cor_target = abs(cor[\"default_ind\"])\n",
    "#Selecting highly correlated features\n",
    "relevant_features = cor_target[cor_target>0.25]\n",
    "relevant_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ghjgI_8ttqUs",
   "metadata": {
    "id": "ghjgI_8ttqUs"
   },
   "outputs": [],
   "source": [
    "def print_highly_correlated(df, features, threshold=0.5):\n",
    "  corr_df = df[features].corr(method='spearman', min_periods = 3) # get correlations\n",
    "  correlated_features = np.where(np.abs(corr_df) > threshold) # select ones above the abs threshold\n",
    "  correlated_features = [(corr_df.iloc[x,y], x, y) for x, y in zip(*correlated_features) if x != y and x < y] # avoid duplication\n",
    "  s_corr_list = sorted(correlated_features, key=lambda x: -abs(x[0])) # sort by correlation value\n",
    "  \n",
    "  if s_corr_list == []:\n",
    "    print(\"There are no highly correlated features with correlation above\", threshold)\n",
    "  else:\n",
    "    for v, i, j in s_corr_list:\n",
    "      cols = df[features].columns\n",
    "      print (\"%s and %s = %.3f\" % (corr_df.index[i], corr_df.columns[j], v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9OJHBrS8viFD",
   "metadata": {
    "id": "9OJHBrS8viFD"
   },
   "outputs": [],
   "source": [
    "print_highly_correlated(train_data, train_data.columns, threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2i2cJ2pLxV-_",
   "metadata": {
    "id": "2i2cJ2pLxV-_"
   },
   "source": [
    "*Variance Inflation factor*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bhyuJN9GxWpE",
   "metadata": {
    "id": "bhyuJN9GxWpE"
   },
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "# Indicate which variables to compute VIF\n",
    "Y = train_data\n",
    "\n",
    "Y.dropna(inplace=True) # vif can't be calculated with nan values\n",
    "Y = Y._get_numeric_data()\n",
    "\n",
    "# Compute VIF\n",
    "vif = pd.DataFrame()\n",
    "vif[\"variables\"] = Y.columns\n",
    "vif[\"VIF\"] = [variance_inflation_factor(Y.values, i) for i in range(Y.shape[1])]\n",
    "vif.sort_values(by='VIF', ascending=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XcPgMrmZrWfI",
   "metadata": {
    "id": "XcPgMrmZrWfI"
   },
   "source": [
    "**Dropping Highly correlated and high VIF columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Gm5TkLcVr64X",
   "metadata": {
    "id": "Gm5TkLcVr64X"
   },
   "outputs": [],
   "source": [
    "df_new2 = train_data.drop(['mvar4', 'mvar5', 'mvar8', 'mvar17', 'mvar18', 'mvar20', 'mvar23', 'mvar27', 'mvar10', \n",
    "                           'mvar32', 'mvar48', 'mvar49', 'mvar34'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SA8A8jUtmOYZ",
   "metadata": {
    "id": "SA8A8jUtmOYZ"
   },
   "source": [
    "**Principal Component Analysis (PCA)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hVAh1RE2rW9n",
   "metadata": {
    "id": "hVAh1RE2rW9n"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X = StandardScaler().fit_transform(df_new2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ExEwnbBvi0ZM",
   "metadata": {
    "id": "ExEwnbBvi0ZM"
   },
   "outputs": [],
   "source": [
    "#Computing the covariance matrix\n",
    "cov = (X.T @ X) / (X.shape[0] - 1)\n",
    "\n",
    "#Performing eigendecomposition\n",
    "eig_values, eig_vectors = np.linalg.eig(cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dmOAJtWj8c77",
   "metadata": {
    "id": "dmOAJtWj8c77"
   },
   "outputs": [],
   "source": [
    "#Determine which principal components to select\n",
    "idx = np.argsort(eig_values, axis=0)[::-1]\n",
    "sorted_eig_vectors = eig_vectors[:, idx]\n",
    "\n",
    "#plotting the cumulative sum of the eigenvalues\n",
    "plt.figure(figsize=(20,8))\n",
    "cumsum = np.cumsum(eig_values[idx]) / np.sum(eig_values[idx])\n",
    "xint = range(1, len(cumsum) + 1)\n",
    "\n",
    "plt.plot(xint, cumsum)\n",
    "\n",
    "plt.xlabel(\"Number of components\")\n",
    "plt.ylabel(\"Cumulative explained variance\")\n",
    "plt.xticks(xint)\n",
    "plt.xlim(1, 40, 2)\n",
    "plt.ylim(0,1,0.2)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-kmltq1jnwHM",
   "metadata": {
    "id": "-kmltq1jnwHM"
   },
   "source": [
    "From the plot, we can see that over 95% of the variance is captured within the **24** largest principal components. Therefore, it is acceptable to choose the **24** largest principal components to make up the projection matrix W."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RUYwIRCGoVra",
   "metadata": {
    "id": "RUYwIRCGoVra"
   },
   "outputs": [],
   "source": [
    "df_new2.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "B4VyzLXBisbt",
   "metadata": {
    "id": "B4VyzLXBisbt"
   },
   "source": [
    "# **Model Building**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1_HV_G9L6SDb",
   "metadata": {
    "id": "1_HV_G9L6SDb"
   },
   "outputs": [],
   "source": [
    "df_new2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5714771",
   "metadata": {
    "id": "f5714771"
   },
   "outputs": [],
   "source": [
    "X = df_new2.drop(['default_ind'], axis = 1)\n",
    "y = df_new2['default_ind']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e0fb12",
   "metadata": {
    "id": "f1e0fb12"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc, average_precision_score\n",
    "from sklearn.metrics import precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MewnLIgOKmKl",
   "metadata": {
    "id": "MewnLIgOKmKl"
   },
   "outputs": [],
   "source": [
    "#df_new2.to_csv(\"cleaned_data.csv\")  # downloaded the data so that we don't waste time in imputation next time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UON_tHrDPh8x",
   "metadata": {
    "id": "UON_tHrDPh8x"
   },
   "source": [
    "#### **Using PyCaret to compare models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LGOOj9JnNesj",
   "metadata": {
    "id": "LGOOj9JnNesj"
   },
   "outputs": [],
   "source": [
    "from pycaret.classification import *\n",
    "\n",
    "clf=setup(data=df_new2,target='default_ind')\n",
    "compare_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4DnyLhJVQCze",
   "metadata": {
    "id": "4DnyLhJVQCze"
   },
   "outputs": [],
   "source": [
    "#xgboost_classifier=create_model('xgboost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "x0qgiiXxQCwx",
   "metadata": {
    "id": "x0qgiiXxQCwx"
   },
   "outputs": [],
   "source": [
    "# Let's now check the model hyperparameters\n",
    "#print(xgboost_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xzz0ZxC5QCt-",
   "metadata": {
    "id": "xzz0ZxC5QCt-"
   },
   "outputs": [],
   "source": [
    "#Hyperparameter Tuning\n",
    "\n",
    "#tuned_xgboost_classifier=tune_model(xgboost_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_oeJCsqMRC3o",
   "metadata": {
    "id": "_oeJCsqMRC3o"
   },
   "outputs": [],
   "source": [
    "#classification report\n",
    "#plot_model(tuned_xgboost_classifier,plot='class_report')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v3hMy74IRC0r",
   "metadata": {
    "id": "v3hMy74IRC0r"
   },
   "outputs": [],
   "source": [
    "#confusion matrix\n",
    "#plot_model(tuned_xgboost_classifier,plot='confusion_matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GhgwjYF5RghH",
   "metadata": {
    "id": "GhgwjYF5RghH"
   },
   "outputs": [],
   "source": [
    "#saving the model for future predictions\n",
    "\n",
    "#save_model(tuned_xgboost_classifier,\"XGBOOST CLASSIFIER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CVL1N-SNRgeI",
   "metadata": {
    "id": "CVL1N-SNRgeI"
   },
   "outputs": [],
   "source": [
    "#loading the saved model\n",
    "\n",
    "#saved_model=load_model('XGBOOST CLASSIFIER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "E3Qu9TXHRun7",
   "metadata": {
    "id": "E3Qu9TXHRun7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SsKRwbaJRukt",
   "metadata": {
    "id": "SsKRwbaJRukt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "q_90s5SH6b1t",
   "metadata": {
    "id": "q_90s5SH6b1t"
   },
   "source": [
    "#### **Dataset Balancing using SMOTETOMEK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a263947",
   "metadata": {
    "id": "7a263947"
   },
   "outputs": [],
   "source": [
    "# Since our classes are highly skewed we should make them equivalent in order to have a normal \n",
    "# distribution of the classes.\n",
    "\n",
    "from imblearn.combine import SMOTETomek \n",
    "smt = SMOTETomek(random_state=42)\n",
    "X_train1, Y_train1 = smt.fit_resample(X_train, Y_train)\n",
    "# X_test1, y_test1 = smt.fit_resample(X_test, y_test)\n",
    "\n",
    "X_train1.shape\n",
    "# X_test1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pzfe79gWNfE6",
   "metadata": {
    "id": "pzfe79gWNfE6"
   },
   "outputs": [],
   "source": [
    "X_test1 = X_test\n",
    "y_test1 = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-Uj2jNVb7zKg",
   "metadata": {
    "id": "-Uj2jNVb7zKg"
   },
   "source": [
    "**Model Comparison**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g6_BDUzXyvwC",
   "metadata": {
    "id": "g6_BDUzXyvwC"
   },
   "outputs": [],
   "source": [
    "from sklearn .metrics import roc_auc_score\n",
    "\n",
    "LightGBM = lgb.LGBMClassifier()\n",
    "LightGBM.fit(X_train1, Y_train1)\n",
    "Y_pred = LightGBM.predict(X_test1)\n",
    "precision, recall, _ = precision_recall_curve(y_test1, Y_pred)\n",
    "score_lgbm = roc_auc_score(y_test1, Y_pred)\n",
    "\n",
    "sgd = linear_model.SGDClassifier(max_iter=5, tol=None)\n",
    "sgd.fit(X_train1, Y_train1)\n",
    "Y_pred = sgd.predict(X_test1)\n",
    "precision, recall, _ = precision_recall_curve(y_test1, Y_pred)\n",
    "score_sgd = roc_auc_score(y_test1, Y_pred)\n",
    "\n",
    "logreg = LogisticRegression(max_iter=10000)\n",
    "logreg.fit(X_train1, Y_train1)\n",
    "Y_pred = logreg.predict(X_test1)\n",
    "precision, recall, _ = precision_recall_curve(y_test1, Y_pred)\n",
    "score_log = roc_auc_score(y_test1, Y_pred)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 3) \n",
    "knn.fit(X_train1, Y_train1)  \n",
    "Y_pred = knn.predict(X_test1)  \n",
    "precision, recall, _ = precision_recall_curve(y_test1, Y_pred)\n",
    "score_knn = roc_auc_score(y_test1, Y_pred)\n",
    "\n",
    "gaussian = GaussianNB() \n",
    "gaussian.fit(X_train1, Y_train1)  \n",
    "Y_pred = gaussian.predict(X_test1)\n",
    "precision, recall, _ = precision_recall_curve(y_test1, Y_pred)\n",
    "score_gaussian = roc_auc_score(y_test1, Y_pred)\n",
    "\n",
    "perceptron = Perceptron(max_iter=100)\n",
    "perceptron.fit(X_train1, Y_train1)\n",
    "Y_pred = perceptron.predict(X_test1)\n",
    "precision, recall, _ = precision_recall_curve(y_test1, Y_pred)\n",
    "score_perceptron = roc_auc_score(y_test1, Y_pred)\n",
    "\n",
    "decision_tree = DecisionTreeClassifier() \n",
    "decision_tree.fit(X_train1, Y_train1)  \n",
    "Y_pred = decision_tree.predict(X_test1)  \n",
    "precision, recall, _ = precision_recall_curve(y_test1, Y_pred)\n",
    "score_decision_tree = roc_auc_score(y_test1, Y_pred)\n",
    "\n",
    "random_forest = RandomForestClassifier(n_estimators=100)\n",
    "random_forest.fit(X_train1, Y_train1)\n",
    "Y_pred = random_forest.predict(X_test1)\n",
    "precision, recall, _ = precision_recall_curve(y_test1, Y_pred)\n",
    "score_random_forest = roc_auc_score(y_test1, Y_pred)\n",
    "\n",
    "AdaBoost = AdaBoostClassifier()\n",
    "AdaBoost.fit(X_train1, Y_train1)\n",
    "Y_pred = AdaBoost.predict(X_test1)\n",
    "precision, recall, _ = precision_recall_curve(y_test1, Y_pred)\n",
    "score_AdaBoost = roc_auc_score(y_test1, Y_pred)\n",
    "\n",
    "XGBoost = XGBClassifier()\n",
    "XGBoost.fit(X_train1, Y_train1)\n",
    "Y_pred = XGBoost.predict(X_test1)\n",
    "precision, recall, _ = precision_recall_curve(y_test1, Y_pred)\n",
    "score_XGBoost = roc_auc_score(y_test1, Y_pred)\n",
    "\n",
    "CatBoost = CatBoostClassifier()\n",
    "CatBoost.fit(X_train1, Y_train1)\n",
    "Y_pred = CatBoost.predict(X_test1)\n",
    "precision, recall, _ = precision_recall_curve(y_test1, Y_pred)\n",
    "score_CatBoost = roc_auc_score(y_test1, Y_pred)\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "MLPClassifier = MLPClassifier()\n",
    "MLPClassifier.fit(X_train1, Y_train1)\n",
    "Y_pred = MLPClassifier.predict(X_test1)\n",
    "precision, recall, _ = precision_recall_curve(y_test1, Y_pred)\n",
    "score_MLPClassifier = roc_auc_score(y_test1, Y_pred)\n",
    "\n",
    "# comparing all the models\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['KNN', 'Logistic Regression','Random Forest', 'Naive Bayes', 'Perceptron', \n",
    "              'Stochastic Gradient Decent','Decision Tree', 'XG Boost', 'AdaBoost', 'Neural Network', 'Cat Boost', 'Light GBM'],\n",
    "    'ROC-AUC': [score_knn, score_log, score_random_forest, score_gaussian, score_perceptron, score_sgd, \n",
    "              score_decision_tree, score_XGBoost, score_AdaBoost, score_MLPClassifier, score_CatBoost, score_lgbm]})\n",
    "result_df = results.sort_values(by='ROC-AUC', ascending=False)\n",
    "result_df = result_df.set_index('ROC-AUC')\n",
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LG-BD5TL7uee",
   "metadata": {
    "id": "LG-BD5TL7uee"
   },
   "source": [
    "#### **Dataset Balancing using SMOTEENN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733116dc",
   "metadata": {
    "id": "733116dc"
   },
   "outputs": [],
   "source": [
    "# Since our classes are highly skewed we should make them equivalent in order to have a normal \n",
    "# distribution of the classes.\n",
    "\n",
    "from imblearn.combine import SMOTEENN \n",
    "sme = SMOTEENN(random_state=42)\n",
    "X_train2, Y_train2 = sme.fit_resample(X_train, Y_train)\n",
    "# X_test2, y_test2 = sme.fit_resample(X_test, y_test)\n",
    "X_test2, y_test2 = X_test, y_test\n",
    "X_train2.shape\n",
    "X_test2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BxwbLSeWQpY6",
   "metadata": {
    "id": "BxwbLSeWQpY6"
   },
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NkBZZ4ITx7jj",
   "metadata": {
    "id": "NkBZZ4ITx7jj"
   },
   "outputs": [],
   "source": [
    "X_res, Y_res = sme.fit_resample(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "URoPPWAv75xV",
   "metadata": {
    "id": "URoPPWAv75xV"
   },
   "source": [
    "**Model Comparison**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EGHSMl_EBX9W",
   "metadata": {
    "id": "EGHSMl_EBX9W"
   },
   "outputs": [],
   "source": [
    "from sklearn .metrics import roc_auc_score\n",
    "\n",
    "LightGBM2 = lgb.LGBMClassifier()\n",
    "LightGBM2.fit(X_train2, Y_train2)\n",
    "Y_pred = LightGBM2.predict(X_test2)\n",
    "precision, recall, _ = precision_recall_curve(y_test2, Y_pred)\n",
    "score_lgbm = roc_auc_score(y_test2, Y_pred)\n",
    "\n",
    "sgd = linear_model.SGDClassifier(max_iter=5, tol=None)\n",
    "sgd.fit(X_train2, Y_train2)\n",
    "Y_pred = sgd.predict(X_test2)\n",
    "precision, recall, _ = precision_recall_curve(y_test2, Y_pred)\n",
    "score_sgd = roc_auc_score(y_test2, Y_pred)\n",
    "\n",
    "logreg = LogisticRegression(max_iter=10000)\n",
    "logreg.fit(X_train2, Y_train2)\n",
    "Y_pred = logreg.predict(X_test2)\n",
    "precision, recall, _ = precision_recall_curve(y_test2, Y_pred)\n",
    "score_log = roc_auc_score(y_test2, Y_pred)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 3) \n",
    "knn.fit(X_train2, Y_train2)  \n",
    "Y_pred = knn.predict(X_test2)  \n",
    "precision, recall, _ = precision_recall_curve(y_test2, Y_pred)\n",
    "score_knn = roc_auc_score(y_test2, Y_pred)\n",
    "\n",
    "gaussian = GaussianNB() \n",
    "gaussian.fit(X_train2, Y_train2)  \n",
    "Y_pred = gaussian.predict(X_test2)\n",
    "precision, recall, _ = precision_recall_curve(y_test2, Y_pred)\n",
    "score_gaussian = roc_auc_score(y_test2, Y_pred)\n",
    "\n",
    "perceptron = Perceptron(max_iter=100)\n",
    "perceptron.fit(X_train2, Y_train2)\n",
    "Y_pred = perceptron.predict(X_test2)\n",
    "precision, recall, _ = precision_recall_curve(y_test2, Y_pred)\n",
    "score_perceptron = roc_auc_score(y_test2, Y_pred)\n",
    "\n",
    "decision_tree = DecisionTreeClassifier() \n",
    "decision_tree.fit(X_train2, Y_train2)  \n",
    "Y_pred = decision_tree.predict(X_test2)  \n",
    "precision, recall, _ = precision_recall_curve(y_test2, Y_pred)\n",
    "score_decision_tree = roc_auc_score(y_test2, Y_pred)\n",
    "\n",
    "random_forest = RandomForestClassifier(n_estimators=100)\n",
    "random_forest.fit(X_train2, Y_train2)\n",
    "Y_pred = random_forest.predict(X_test2)\n",
    "precision, recall, _ = precision_recall_curve(y_test2, Y_pred)\n",
    "score_random_forest = roc_auc_score(y_test2, Y_pred)\n",
    "\n",
    "AdaBoost = AdaBoostClassifier()\n",
    "AdaBoost.fit(X_train2, Y_train2)\n",
    "Y_pred = AdaBoost.predict(X_test2)\n",
    "precision, recall, _ = precision_recall_curve(y_test2, Y_pred)\n",
    "score_AdaBoost = roc_auc_score(y_test2, Y_pred)\n",
    "\n",
    "XGBoost = XGBClassifier()\n",
    "XGBoost.fit(X_train2, Y_train2)\n",
    "Y_pred = XGBoost.predict(X_test2)\n",
    "precision, recall, _ = precision_recall_curve(y_test2, Y_pred)\n",
    "score_XGBoost = roc_auc_score(y_test2, Y_pred)\n",
    "\n",
    "CatBoost2 = CatBoostClassifier()\n",
    "CatBoost2.fit(X_train2, Y_train2)\n",
    "Y_pred = CatBoost2.predict(X_test2)\n",
    "precision, recall, _ = precision_recall_curve(y_test2, Y_pred)\n",
    "score_CatBoost = roc_auc_score(y_test2, Y_pred)\n",
    "\n",
    "# comparing all the models\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['KNN', 'Logistic Regression','Random Forest', 'Naive Bayes', 'Perceptron', \n",
    "              'Stochastic Gradient Decent','Decision Tree', 'XG Boost', 'AdaBoost', 'Cat Boost', 'Light GBM'],\n",
    "    'ROC-AUC': [score_knn, score_log, score_random_forest, score_gaussian, score_perceptron, score_sgd, \n",
    "              score_decision_tree, score_XGBoost, score_AdaBoost, score_CatBoost, score_lgbm]})\n",
    "result_df = results.sort_values(by='ROC-AUC', ascending=False)\n",
    "result_df = result_df.set_index('ROC-AUC')\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Fv5fJt5EK4Ar",
   "metadata": {
    "id": "Fv5fJt5EK4Ar"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_true = y_test2\n",
    "y_pred = CatBoost2.predict(X_test2)\n",
    "print('Catboost_model: recall:', recall_score(y_true, y_pred))\n",
    "print('Catboost_model: precision:',precision_score(y_true, y_pred))\n",
    "print('catboost_model: F1 Score:', f1_score(y_true, y_pred))\n",
    "print('Catboost_model: accuracy:', accuracy_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LxWOsavgMouK",
   "metadata": {
    "id": "LxWOsavgMouK"
   },
   "outputs": [],
   "source": [
    "#Hyperparameter tuning of catboost\n",
    "from scipy.stats import randint\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "cbc = CatBoostClassifier()\n",
    "\n",
    "# Creating the hyperparameter grid\n",
    "param_dist = { \"learning_rate\": np.linspace(0,0.2,5),\n",
    "               \"max_depth\": randint(3, 10)}\n",
    "               \n",
    "#Instantiate RandomSearchCV object\n",
    "rscv = RandomizedSearchCV(cbc , param_dist, scoring='roc_auc', cv =5)\n",
    "\n",
    "#Fit the model\n",
    "rscv.fit(X_train2,Y_train2)\n",
    "\n",
    "# Print the tuned parameters and score\n",
    "print(rscv.best_params_)\n",
    "print(rscv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "h78zi5rcRKlT",
   "metadata": {
    "id": "h78zi5rcRKlT"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_true = y_test2\n",
    "y_pred = rscv.predict(X_test2)\n",
    "print('Catboost_model: recall:', recall_score(y_true, y_pred))\n",
    "print('Catboost_model: precision:',precision_score(y_true, y_pred))\n",
    "print('catboost_model: F1 Score:', f1_score(y_true, y_pred))\n",
    "print('Catboost_model: accuracy:', accuracy_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WrNsU4MbjqpR",
   "metadata": {
    "id": "WrNsU4MbjqpR"
   },
   "source": [
    "# **Working on COMPANY PROVIDED TEST DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb9a9cb",
   "metadata": {
    "id": "5eb9a9cb"
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"testX.csv\")\n",
    "test_df1 = test_df.drop(['mvar47'], axis=1)\n",
    "test_df2 = test_df1.replace(to_replace =\"[a-zA-Z]+\", value = np.nan, regex = True)\n",
    "test_df2 = test_df2.astype('float')\n",
    "test_df2 = pd.concat([test_df2, test_df['mvar47']], axis=1)\n",
    "test_df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GiapvbJ_DPV5",
   "metadata": {
    "id": "GiapvbJ_DPV5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4LLRHCUqDS3Q",
   "metadata": {
    "id": "4LLRHCUqDS3Q"
   },
   "outputs": [],
   "source": [
    "test_df2['mvar16'][test_df2['mvar16'].isnull()] = -1\n",
    "test_df2['mvar17'][test_df2['mvar17'].isnull()] = -1\n",
    "test_df2['mvar18'][test_df2['mvar18'].isnull()] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pJHwEZQeDS3R",
   "metadata": {
    "id": "pJHwEZQeDS3R"
   },
   "outputs": [],
   "source": [
    "test_df2['mvar16'] = test_df2['mvar16']+test_df2['mvar17']+test_df2['mvar18']\n",
    "\n",
    "test_df2['mvar16'][test_df2['mvar16']>1] = test_df2['mvar16'][test_df2['mvar16']>1] //5\n",
    "test_df2['mvar16'] = test_df2['mvar16'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_QCJezRpDS3S",
   "metadata": {
    "id": "_QCJezRpDS3S"
   },
   "outputs": [],
   "source": [
    "#mvar26\n",
    "\n",
    "test_df2['mvar26'][test_df2['mvar26'].isnull()==False]  = ((test_df2['mvar26'][test_df2['mvar26'].isnull()==False]).astype('int64'))//12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kYgst89HDS3S",
   "metadata": {
    "id": "kYgst89HDS3S"
   },
   "outputs": [],
   "source": [
    "# df['mvar26'][df['mvar26'].isnull()] = np.mean(df['mvar26'])\n",
    "test_df2['mvar26'][test_df2['mvar26'].isnull()] = -365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "n-0SQdLvDS3T",
   "metadata": {
    "id": "n-0SQdLvDS3T"
   },
   "outputs": [],
   "source": [
    "test_df2['mvar26'] = test_df2['mvar26'].astype('int64')\n",
    "test_df2['mvar26'] = test_df2['mvar26']//365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dmTz7XaiDS3T",
   "metadata": {
    "id": "dmTz7XaiDS3T"
   },
   "outputs": [],
   "source": [
    "# mvar27\n",
    "\n",
    "test_df2['mvar27'][test_df2['mvar27'].isnull()] = -365\n",
    "test_df2['mvar27'] = test_df2['mvar27'] //365\n",
    "test_df2['mvar27'] = test_df2['mvar27'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0TEDh3iVDS3U",
   "metadata": {
    "id": "0TEDh3iVDS3U"
   },
   "outputs": [],
   "source": [
    "#mvar26+27\n",
    "\n",
    "test_df2['mvar27'][test_df2['mvar27'].isnull()] = -365\n",
    "test_df2['mvar26'] = (test_df2['mvar26']+test_df2['mvar27'])/2/365\n",
    "test_df2['mvar26'] = test_df2['mvar26'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-mI99vaSDS3U",
   "metadata": {
    "id": "-mI99vaSDS3U"
   },
   "outputs": [],
   "source": [
    "#mvar28\n",
    "\n",
    "test_df2['mvar28'][test_df2['mvar28'].isnull()] = -365\n",
    "test_df2['mvar28'] = test_df2['mvar28'] //365\n",
    "test_df2['mvar28'] = test_df2['mvar28'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3SiPr7tDS3U",
   "metadata": {
    "id": "b3SiPr7tDS3U"
   },
   "outputs": [],
   "source": [
    "#mvar30\n",
    "\n",
    "test_df2['mvar30'][test_df2['mvar30'].isnull()] = -365\n",
    "test_df2['mvar30'] = test_df2['mvar30'] //365\n",
    "test_df2['mvar30'] = test_df2['mvar30'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Olm6MGxvDS3U",
   "metadata": {
    "id": "Olm6MGxvDS3U"
   },
   "outputs": [],
   "source": [
    "# mvar31\n",
    "\n",
    "test_df2['mvar31'][test_df2['mvar31'].isnull()] = -365\n",
    "test_df2['mvar31'] = test_df2['mvar31'] //365\n",
    "test_df2['mvar31'] = test_df2['mvar31'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PxSbXVw7DS3V",
   "metadata": {
    "id": "PxSbXVw7DS3V"
   },
   "outputs": [],
   "source": [
    "test_df2['mvar30'] = test_df2[[\"mvar30\", \"mvar31\"]].max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ECQjfhq_DS3V",
   "metadata": {
    "id": "ECQjfhq_DS3V"
   },
   "outputs": [],
   "source": [
    "test_df2['mvar30'][test_df2['mvar30'].isnull()] = -365\n",
    "test_df2['mvar30'] = test_df2['mvar30'] //365\n",
    "test_df2['mvar30'] = test_df2['mvar30'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9trBdhIyDS3V",
   "metadata": {
    "id": "9trBdhIyDS3V"
   },
   "outputs": [],
   "source": [
    "#mvar33\n",
    "\n",
    "test_df2['mvar33'][test_df2['mvar33'].isnull()] = -5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fj49KKxqDS3W",
   "metadata": {
    "id": "fj49KKxqDS3W"
   },
   "outputs": [],
   "source": [
    "test_df2['mvar33'] = test_df2['mvar33'] //5\n",
    "test_df2['mvar33'] = test_df2['mvar33'].round(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XMl0wJlgDS3W",
   "metadata": {
    "id": "XMl0wJlgDS3W"
   },
   "outputs": [],
   "source": [
    "#mvar34\n",
    "\n",
    "test_df2['mvar34'][test_df2['mvar34'].isnull()] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gFiDvBtYDS3W",
   "metadata": {
    "id": "gFiDvBtYDS3W"
   },
   "outputs": [],
   "source": [
    "#mvar35\n",
    "\n",
    "test_df2['mvar35'][test_df2['mvar35'].isnull()] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "axuAetyxDS3W",
   "metadata": {
    "id": "axuAetyxDS3W"
   },
   "outputs": [],
   "source": [
    "test_df2['mvar35'] = test_df2['mvar35'] + test_df2['mvar34']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wa2a04uhDS3X",
   "metadata": {
    "id": "wa2a04uhDS3X"
   },
   "outputs": [],
   "source": [
    "#mvar36\n",
    "\n",
    "test_df2['mvar36'][test_df2['mvar36'].isnull()] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yWB2OpCvDS3X",
   "metadata": {
    "id": "yWB2OpCvDS3X"
   },
   "outputs": [],
   "source": [
    "#mvar37\n",
    "\n",
    "test_df2['mvar37'][test_df2['mvar37'].isnull()] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gzexmTajDS3X",
   "metadata": {
    "id": "gzexmTajDS3X"
   },
   "outputs": [],
   "source": [
    "#mvar38\n",
    "\n",
    "test_df2['mvar38'][test_df2['mvar38'].isnull()] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5TCvarC-DS3X",
   "metadata": {
    "id": "5TCvarC-DS3X"
   },
   "outputs": [],
   "source": [
    "#mvar39\n",
    "\n",
    "test_df2['mvar39'][test_df2['mvar39'].isnull()] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OOCRw97QDS3Y",
   "metadata": {
    "id": "OOCRw97QDS3Y"
   },
   "outputs": [],
   "source": [
    "#mvar40\n",
    "\n",
    "binn = 50\n",
    "test_df2['mvar40'][test_df2['mvar40'].isnull()] = -binn\n",
    "test_df2['mvar40'] = test_df2['mvar40'] /binn\n",
    "test_df2['mvar40'] = test_df2['mvar40'].round(0)\n",
    "test_df2['mvar40'] = test_df2['mvar40'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-h1b6YW_DS3Y",
   "metadata": {
    "id": "-h1b6YW_DS3Y"
   },
   "outputs": [],
   "source": [
    "#mvar41\n",
    "\n",
    "binn = 25\n",
    "test_df2['mvar41'][test_df2['mvar41'].isnull()] = -binn\n",
    "test_df2['mvar41'] = test_df2['mvar41'] /binn\n",
    "test_df2['mvar41'] = test_df2['mvar41'].round(0)\n",
    "test_df2['mvar41'] = test_df2['mvar41'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TkZ-Z2SJDS3Y",
   "metadata": {
    "id": "TkZ-Z2SJDS3Y"
   },
   "outputs": [],
   "source": [
    "#mvar45\n",
    "\n",
    "# binn = 25\n",
    "test_df2['mvar45'][test_df2['mvar45'].isnull()] = -1\n",
    "# df2['mvar41'] = df2['mvar41'] /binn\n",
    "# df2['mvar41'] = df2['mvar41'].round(0)\n",
    "# df2['mvar41'] = df2['mvar41'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "slRxxMRkDS3Y",
   "metadata": {
    "id": "slRxxMRkDS3Y"
   },
   "outputs": [],
   "source": [
    "#mvar46\n",
    "\n",
    "# binn = 25\n",
    "test_df2['mvar46'][test_df2['mvar46'].isnull()] = -1\n",
    "# df2['mvar41'] = df2['mvar41'] /binn\n",
    "# df2['mvar41'] = df2['mvar41'].round(0)\n",
    "# df2['mvar41'] = df2['mvar41'].astype('int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XZLrTeo8ETGU",
   "metadata": {
    "id": "XZLrTeo8ETGU"
   },
   "source": [
    "**One hot encoding to convert categorical feature to numerical one**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MErji3jxETGU",
   "metadata": {
    "id": "MErji3jxETGU"
   },
   "outputs": [],
   "source": [
    "test_df2 = pd.get_dummies(test_df2, columns = ['mvar47'])\n",
    "test_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KSqgExU0EbeK",
   "metadata": {
    "id": "KSqgExU0EbeK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7UwANEOSETGS",
   "metadata": {
    "id": "7UwANEOSETGS"
   },
   "outputs": [],
   "source": [
    "null_df2 = pd.DataFrame((test_df2.isnull().sum())*100/test_df2.shape[0]).reset_index()\n",
    "null_df2.columns = ['Column Name', 'Null Values Percentage']\n",
    "\n",
    "# more than or equal to 40% empty rows columns\n",
    "nullcol_50_df2 = null_df2[null_df2[\"Null Values Percentage\"]>=50]\n",
    "nullcol_50_df2.sort_values(by='Null Values Percentage', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01A60RYhETGT",
   "metadata": {
    "id": "01A60RYhETGT"
   },
   "outputs": [],
   "source": [
    "# dropping columns which have > 70% NULL values\n",
    "#test_df3 = test_df2.drop(['mvar31', 'mvar40', 'mvar41'], axis=1)\n",
    "test_df3 = test_df2.drop('application_key', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AbHNN9aCETGU",
   "metadata": {
    "id": "AbHNN9aCETGU"
   },
   "source": [
    "**Scaling and normalisation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WUb0Y_hWETGV",
   "metadata": {
    "id": "WUb0Y_hWETGV"
   },
   "source": [
    "*Standardization*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mhpc-fQmETGV",
   "metadata": {
    "id": "mhpc-fQmETGV"
   },
   "outputs": [],
   "source": [
    "# importing sklearn StandardScaler class which is for Standardization\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#sc = StandardScaler() # creating an instance of the class object\n",
    "#df_scaled = pd.DataFrame(sc.fit_transform(train_df4), columns=train_df4.columns)  #fit and transforming StandardScaler the dataframe "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g-K9fUG2ETGV",
   "metadata": {
    "id": "g-K9fUG2ETGV"
   },
   "source": [
    "*Min-Max Scaling*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_aB_dhaHETGV",
   "metadata": {
    "id": "_aB_dhaHETGV"
   },
   "outputs": [],
   "source": [
    "# importing sklearn Min Max Scaler class which is for Standardization\n",
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#mm = MinMaxScaler() # creating an instance of the class object\n",
    "#df_scaled = pd.DataFrame(mm.fit_transform(train_df4), columns=train_df4.columns)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-0YhKRXyETGV",
   "metadata": {
    "id": "-0YhKRXyETGV"
   },
   "source": [
    "*Max Absolute Scaling*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9gBjliJzETGV",
   "metadata": {
    "id": "9gBjliJzETGV"
   },
   "outputs": [],
   "source": [
    "#from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "#ma = MaxAbsScaler() # creating an instance of the class object\n",
    "#df_scaled = pd.DataFrame(ma.fit_transform(train_df4), columns=train_df4.columns)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UKteLShLETGW",
   "metadata": {
    "id": "UKteLShLETGW"
   },
   "source": [
    "*Robust Scaling*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "j033eKyVETGW",
   "metadata": {
    "id": "j033eKyVETGW"
   },
   "outputs": [],
   "source": [
    "# importing sklearn Min Max Scaler class which is for Robust scaling\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "rs = RobustScaler() # creating an instance of the class object\n",
    "X = rs.fit_transform(test_df3)\n",
    "testdf_scaled = pd.DataFrame(X, columns=test_df3.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9RqlJsGrETGW",
   "metadata": {
    "id": "9RqlJsGrETGW"
   },
   "source": [
    "**Imputing NULL values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tqTYoyZn1T_y",
   "metadata": {
    "id": "tqTYoyZn1T_y"
   },
   "outputs": [],
   "source": [
    "df_scaled3 = testdf_scaled.drop(['mvar39'], axis=1)\n",
    "df_scaled4 = testdf_scaled[['mvar39']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FxLG63_r1T_7",
   "metadata": {
    "id": "FxLG63_r1T_7"
   },
   "outputs": [],
   "source": [
    "result = df_scaled4.fillna(df_scaled4.mode().iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lqQXiE2k1T_7",
   "metadata": {
    "id": "lqQXiE2k1T_7"
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=3)\n",
    "imputed = imputer.fit_transform(df_scaled3)\n",
    "test_data1 = pd.DataFrame(imputed, columns=df_scaled3.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pa81Dl2t1T_8",
   "metadata": {
    "id": "pa81Dl2t1T_8"
   },
   "outputs": [],
   "source": [
    "test_data = pd.concat([test_data1, result], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pMA5BpfCETGW",
   "metadata": {
    "id": "pMA5BpfCETGW"
   },
   "outputs": [],
   "source": [
    "test_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afb6ln7ETGY",
   "metadata": {
    "id": "8afb6ln7ETGY"
   },
   "source": [
    "**Dropping Highly correlated and high VIF columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nf_3wTfVETGY",
   "metadata": {
    "id": "nf_3wTfVETGY"
   },
   "outputs": [],
   "source": [
    "testdf_new2 = test_data.drop(['mvar4', 'mvar5', 'mvar8', 'mvar17', 'mvar18', 'mvar20', 'mvar23', 'mvar27', 'mvar10', \n",
    "                           'mvar32', 'mvar48', 'mvar49', 'mvar34'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7g4RvZKmEOvZ",
   "metadata": {
    "id": "7g4RvZKmEOvZ"
   },
   "outputs": [],
   "source": [
    "testdf_new2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e61ed9",
   "metadata": {
    "id": "62e61ed9"
   },
   "outputs": [],
   "source": [
    "# make predictions for test data\n",
    "X_test = testdf_new2\n",
    "y_pred = rscv.predict(X_test)\n",
    "\n",
    "new = pd.DataFrame(y_pred, columns=['predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RY3Vt8bY9rm3",
   "metadata": {
    "id": "RY3Vt8bY9rm3"
   },
   "outputs": [],
   "source": [
    "new.insert(0, \"application_key\", test_df['application_key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "w2_IMZ-9_9qm",
   "metadata": {
    "id": "w2_IMZ-9_9qm"
   },
   "outputs": [],
   "source": [
    "new = new.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "T-61vOiE9zf7",
   "metadata": {
    "id": "T-61vOiE9zf7"
   },
   "outputs": [],
   "source": [
    "new.to_csv('Daring_souls_8.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "q_90s5SH6b1t",
    "LG-BD5TL7uee",
    "WrNsU4MbjqpR"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
